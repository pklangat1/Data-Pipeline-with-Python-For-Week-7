{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project -- Data Pipelines with **Python**"
      ],
      "metadata": {
        "id": "fnijtqkS493G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Deliverables\n",
        "\n"
      ],
      "metadata": {
        "id": "tdJGJjDW5IPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Telecom companies often have to extract billing data from multiple CSV files generated from\n",
        "various systems and transform it into a structured format for analysis and revenue reporting.\n",
        "This process can be time-consuming, error-prone, and hinder decision-making. Manually\n",
        "analyzing and reconciling billing data from different sources is a tedious task and often leads to\n",
        "delays in generating revenue reports.<br><br>Thus, there is a need for an automated data pipeline that\n",
        "can extract billing data from multiple sources and transform it into a structured format for\n",
        "efficient analysis and revenue reporting."
      ],
      "metadata": {
        "id": "WA0pBJL_9NTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Determine the requirements"
      ],
      "metadata": {
        "id": "yAIVcnFY9m1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source of the data: Here are three sample datasets (https://bit.ly/416WE1X) with billing data.\n",
        "\n",
        "Destination of the data: A local csv database\n",
        "\n",
        "The transformations that need to be applied:\n",
        "<br>  1.The datasets can be joined using Customer ID, Date of purchase/payment/refund, and country of purchase/payment/refund as keys.\n",
        "<br>2. The datasets may contain missing values and outliers for some fields, such as the total amount billed or refund amount.\n",
        "<br>3. The payment status may be missing or incomplete for some of the transactions.\n",
        "<br>4. The promo code field may be empty for some of the purchases.\n",
        "<br>5. The reason for the refund may be missing for some of the refund transactions.\n"
      ],
      "metadata": {
        "id": "TQ5Fd7di9yoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Extract the data"
      ],
      "metadata": {
        "id": "CZx386rJAaNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import logging\n",
        "\n",
        "#Setup logger\n",
        "logging.basicConfig(filename='pipeline.log', level=logging.DEBUG)\n",
        "\n",
        "#Initialize the dataframes to be used\n",
        "dset1 = pd.DataFrame()\n",
        "dset2 = pd.DataFrame()\n",
        "dset3 = pd.DataFrame()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cLFHuqwlcQhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use Python to read the CSV files and extract the data.\n",
        "\n",
        "\n",
        "def readCsv(f1, f2, f3):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to read data from csv files and return df for each file\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    dset1 = pd.read_csv(f1)\n",
        "    dset2 = pd.read_csv(f2)\n",
        "    dset3 = pd.read_csv(f3)\n",
        "\n",
        "  except Exception as e:\n",
        "    logging.debug(\"Method readCsv exception occurred\")\n",
        "  \n",
        "  return dset1,dset2,dset3\n",
        "\n"
      ],
      "metadata": {
        "id": "QnV5hkzhApFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset1, dset2,dset3 = readCsv('dataset1.csv','dataset2.csv', 'dataset3.csv' )\n",
        "#dset1.shape"
      ],
      "metadata": {
        "id": "zi-5FxfaBzv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Clean the data"
      ],
      "metadata": {
        "id": "VRSwqX5tCK3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform data cleaning on the extracted data to remove any missing\n",
        "values and outliers. \n",
        "For example, we can replace missing values with an appropriate\n",
        "value or remove them altogether"
      ],
      "metadata": {
        "id": "WWX9T212CQji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def cleanDF(df):\n",
        "  \"\"\"\n",
        "  Function to fix nulls in dset1 promo column and return clean df\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    dset11 = df.fillna('PROMO0')\n",
        "\n",
        "  except Exception as e:\n",
        "    logging.debug(\"Method cleanDF exception occurred\")\n",
        "  \n",
        "  return dset11\n",
        "\n",
        "dset11 = cleanDF(dset1)"
      ],
      "metadata": {
        "id": "dkwW5Y4MCle8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset11.shape\n",
        "dset11.isnull().sum()\n"
      ],
      "metadata": {
        "id": "RVzlLpQhE1Nl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc03649-16e6-4b06-e69a-a4e62e4023c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "customer_id            0\n",
              "date_of_purchase       0\n",
              "total_amount_billed    0\n",
              "payment_status         0\n",
              "payment_method         0\n",
              "promo_code             0\n",
              "country_of_purchase    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset2.isnull().sum()"
      ],
      "metadata": {
        "id": "l8Jjg6lHEZ8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c006d4-bf07-4c53-dce0-d26ad3e03217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "customer_id           0\n",
              "date_of_payment       0\n",
              "amount_paid           0\n",
              "payment_method        0\n",
              "payment_status        0\n",
              "late_payment_fee      0\n",
              "country_of_payment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset3.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TY_1OdnEt9x",
        "outputId": "a6a14138-c01b-4021-f167-6da29e270296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "customer_id          0\n",
              "date_of_refund       0\n",
              "refund_amount        0\n",
              "reason_for_refund    0\n",
              "country_of_refund    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Transform the data "
      ],
      "metadata": {
        "id": "7XYBmuVYCirs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply any necessary transformations on the data, such as data\n",
        "type conversion, data aggregation, and data filtering, to prepare the data for analysis"
      ],
      "metadata": {
        "id": "VIxXNxIaCr6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def aggregator(dset11, dset2, dset3):\n",
        "  \"\"\"\n",
        "  Data aggregation function to get average spend. Returns df + new csv file created with average billed, paid, refunded\n",
        "  \"\"\"\n",
        "  try:\n",
        "    mean_billed_df = dset11.groupby('country_of_purchase')['total_amount_billed'].mean()\n",
        "    mean_paid_df = dset2.groupby('country_of_payment')['amount_paid'].mean()\n",
        "    mean_refund_df = dset3.groupby('country_of_refund')['refund_amount'].mean()\n",
        "\n",
        "    frame = { 'Average_Billed': mean_billed_df, 'Average_Paid': mean_paid_df, 'Average_Refund':mean_refund_df  }\n",
        "\n",
        "    mean_df = pd.DataFrame(frame)\n",
        "    mean_df.to_csv(\"means1.csv\", index=True)\n",
        "\n",
        "  except Exception as e:\n",
        "    logging.debug(\"Method aggregator exception occurred\")\n",
        "\n",
        "  return mean_df\n",
        "\n",
        "mean = aggregator(dset11, dset2, dset3)\n",
        "mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "y3Bcu4E-XYDO",
        "outputId": "016a8e45-61c4-48c3-8c5e-044ce0750114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Average_Billed  Average_Paid  Average_Refund\n",
              "UK        95.833333     95.833333      100.000000\n",
              "USA      108.333333    108.333333      105.555556"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ef7fd2f-1cc3-4956-aeb4-3af127e7a72d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Average_Billed</th>\n",
              "      <th>Average_Paid</th>\n",
              "      <th>Average_Refund</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>UK</th>\n",
              "      <td>95.833333</td>\n",
              "      <td>95.833333</td>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>USA</th>\n",
              "      <td>108.333333</td>\n",
              "      <td>108.333333</td>\n",
              "      <td>105.555556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ef7fd2f-1cc3-4956-aeb4-3af127e7a72d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8ef7fd2f-1cc3-4956-aeb4-3af127e7a72d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8ef7fd2f-1cc3-4956-aeb4-3af127e7a72d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transformDF(dset11, dset2, dset3):\n",
        "\n",
        "  \"\"\" \n",
        "  Data conversion for these columns on dataset1:date_of_purchase, payment_status,\tpayment_method,\tpromo_code,\tcountry_of_purchase\n",
        "  Data conversion for these columns on dataset2:date_of_payment, payment_method , payment_status, country_of_payment \n",
        "  Data conversion for these columns on dataset3:date_of_refund, reason_for_refund, country_of_refund \n",
        "  Returns 3 cleaned dfs \n",
        "  \"\"\"\n",
        "  try:\n",
        "\n",
        "    #dset11\n",
        "    toCategoryType = ['payment_status',\t'payment_method',\t'promo_code',\t'country_of_purchase']\n",
        "\n",
        "    for c in toCategoryType:\n",
        "      dset11[c] = dset11[c].astype(\"category\") \n",
        "\n",
        "    dset11['date_of_purchase'] = pd.to_datetime(dset11['date_of_purchase'])\n",
        "\n",
        "    #dset2\n",
        "    toCategoryType = [ 'payment_method' , 'payment_status', 'country_of_payment' ]\n",
        "\n",
        "    for c in toCategoryType:\n",
        "      dset2[c] = dset2[c].astype(\"category\") \n",
        "\n",
        "    dset2['date_of_payment'] = pd.to_datetime(dset2['date_of_payment'])\n",
        "\n",
        "    #dset3\n",
        "    toCategoryType = [ 'reason_for_refund', 'country_of_refund'  ]\n",
        "\n",
        "    for c in toCategoryType:\n",
        "      dset3[c] = dset3[c].astype(\"category\") \n",
        "\n",
        "    dset3['date_of_refund'] = pd.to_datetime(dset3['date_of_refund'])\n",
        "\n",
        "    #Country column in all 3 datasets identical. Drop 2 and retain 1\n",
        "\n",
        "    dset3 = dset3.drop(columns=['country_of_refund'])\n",
        "    dset2 = dset2.drop(columns=['country_of_payment'])\n",
        "\n",
        "    #rename remaining country column\n",
        "    dset11 = dset11.rename(columns={\"country_of_purchase\": \"country\"})\n",
        "\n",
        "  except Exception as e:\n",
        "    logging.debug(\"Method transformDF exception occurred\")\n",
        "\n",
        "  return dset11, dset2, dset3\n"
      ],
      "metadata": {
        "id": "B11mqWl2k9eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset11, dset2, dset3 = transformDF(dset11, dset2, dset3)"
      ],
      "metadata": {
        "id": "fWJHhtuSQFRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Merge the datasets"
      ],
      "metadata": {
        "id": "3jV14YttL740"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join the different datasets into a single dataset that can be used for\n",
        "analysis."
      ],
      "metadata": {
        "id": "Y-cmgc0jL4vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mergeDF(dset11, dset2, dset3):\n",
        "  \"\"\"\n",
        "  Join the 3 dfs based on customer id column and return merged df\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    df_merge = pd.merge(dset11, dset2, how='inner', on = 'customer_id')\n",
        "    df_merge = pd.merge(df_merge, dset3, how='inner', on = 'customer_id')\n",
        "\n",
        "  except Exception as e:\n",
        "    logging.debug(\"Method mergeDF exception occurred\")\n",
        "\n",
        "  return df_merge\n",
        "\n",
        "df_merge = mergeDF(dset11, dset2, dset3)"
      ],
      "metadata": {
        "id": "5nsYMURQMnFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Load the dataset"
      ],
      "metadata": {
        "id": "5XFCQ7j0OrIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the transformed data into a database or a file, such as a CSV file,\n",
        "that can be easily analyzed."
      ],
      "metadata": {
        "id": "H_vMXb-XOylf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merge.to_csv(\"merge1.csv\", index=False)"
      ],
      "metadata": {
        "id": "RcDFUJ5ZO8NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Automate the process"
      ],
      "metadata": {
        "id": "muTCdW6dUI-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automate the data pipeline by scheduling it to run at a specific\n",
        "time, such as daily or weekly so that it can update the analysis data automatically.\n"
      ],
      "metadata": {
        "id": "HV4Gb3T5UOUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from os import path\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    #Extract Data\n",
        "    dset1, dset2,dset3 = readCsv('dataset1.csv','dataset2.csv', 'dataset3.csv' )\n",
        "\n",
        "    #Use assert test to confirm only dataframes returned\n",
        "    assert(type(dset1).__name__ == 'DataFrame')\n",
        "    assert(type(dset2).__name__ == 'DataFrame')\n",
        "    assert(type(dset3).__name__ == 'DataFrame')\n",
        "\n",
        "    #Clean the data \n",
        "    dset11 = cleanDF(dset1)\n",
        "\n",
        "    #Test that a df has been returned \n",
        "    assert(type(dset11).__name__ == 'DataFrame')\n",
        "\n",
        "    #-Transfrom Data --\n",
        "    #Aggregation\n",
        "    mean = aggregator(dset11, dset2, dset3)\n",
        "    assert(type(mean).__name__ == 'DataFrame')\n",
        "\n",
        "    #Data types conversion and column renaming\n",
        "    dset11, dset2, dset3 = transformDF(dset11, dset2, dset3)\n",
        "    assert(type(dset11).__name__ == 'DataFrame')\n",
        "    assert(type(dset2).__name__ == 'DataFrame')\n",
        "    assert(type(dset3).__name__ == 'DataFrame')\n",
        "\n",
        "    #Merge the datasets\n",
        "    df_merge = mergeDF(dset11, dset2, dset3)\n",
        "    assert(type(df_merge).__name__ == 'DataFrame')\n",
        "\n",
        "    #Load data to csv\n",
        "    df_merge.to_csv(\"merge1.csv\", index=False)\n",
        "\n",
        "    #Check that df contents have been loaded to csv file \n",
        "    assert True is (path.isfile(\"merge1.csv\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "fIMPsVu9U-IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Use steps below to setup cron job.\n",
        "This cron job will execute above pipeline script every Monday at 9am.\n",
        "\n",
        "1- run 'crontab -e'\n",
        "<br>2- append below text, save and close file \n",
        "\n",
        "0 9 * * 1 /path/to/this/pipelinescript.py # Run Mondays 0900hrs\n"
      ],
      "metadata": {
        "id": "Q4hVIyXwg_25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Test the pipeline"
      ],
      "metadata": {
        "id": "hf8p53TwURD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tested the data pipeline to ensure it produces the correct results by using assertion to confirm expected output*\n"
      ],
      "metadata": {
        "id": "AfWQwlYyUb41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Optimize the pipeline"
      ],
      "metadata": {
        "id": "SZke6iKWUt1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Optimized the pipeline using data conversion e.g: switching object types to category type where feasible*\n"
      ],
      "metadata": {
        "id": "KgiryN3gU1Vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. Monitor the pipeline"
      ],
      "metadata": {
        "id": "GhjfirjEU4TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This was implemented using try/except code blocks within the functions.\n",
        "Logging to file was also enabled*"
      ],
      "metadata": {
        "id": "B0XjZHM6U8Wn"
      }
    }
  ]
}